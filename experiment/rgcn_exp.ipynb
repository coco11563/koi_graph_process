{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/tmp/pycharm_project_754\")\n",
    "base = \"/tmp/pycharm_project_754\"\n",
    "data_base = base + '/data/'\n",
    "out_base = base + '/out/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "data file is not existed, download now\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import scipy.io\n",
    "import urllib.request\n",
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "data_url = 'https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/ACM.mat'\n",
    "data_file_path = '/tmp/ACM.mat'\n",
    "if not os.path.exists(data_file_path) : \n",
    "    print(\"data file is not existed, download now\")\n",
    "    urllib.request.urlretrieve(data_url, data_file_path)\n",
    "    copyfile(data_file_path, data_base + 'test/ACM.mat')  \n",
    "print(\"data file prepare done\")\n",
    "data = scipy.io.loadmat(data_base + 'test/ACM.mat')\n",
    "print(list(data.keys()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "<class 'scipy.sparse.csc.csc_matrix'>\n",
      "#Papers: 12499\n",
      "#Authors: 17431\n",
      "#Links: 37055\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(type(data['PvsA']))\n",
    "print('#Papers:', data['PvsA'].shape[0])\n",
    "print('#Authors:', data['PvsA'].shape[1])\n",
    "print('#Links:', data['PvsA'].nnz)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# build heterograph\n",
    "import dgl\n",
    "pa_g = dgl.heterograph({('paper', 'written-by', 'author') : data['PvsA']})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Node types: ['paper', 'author']\n",
      "Edge types: ['written-by']\n",
      "Canonical edge types: [('paper', 'written-by', 'author')]\n",
      "12499\n",
      "37055\n",
      "37055\n",
      "tensor([3532, 6421, 8516, 8560])\n",
      "37055\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print('Node types:', pa_g.ntypes)\n",
    "print('Edge types:', pa_g.etypes)\n",
    "print('Canonical edge types:', pa_g.canonical_etypes)\n",
    "\n",
    "# Nodes and edges are assigned integer IDs starting from zero and each type has its own counting.\n",
    "# To distinguish the nodes and edges of different types, specify the type name as the argument.\n",
    "print(pa_g.number_of_nodes('paper'))\n",
    "# Canonical edge type name can be shortened to only one edge type name if it is\n",
    "# uniquely distinguishable.\n",
    "print(pa_g.number_of_edges(('paper', 'written-by', 'author')))\n",
    "print(pa_g.number_of_edges('written-by'))\n",
    "print(pa_g.successors(1, etype='written-by'))  # get the authors that write paper #1\n",
    "\n",
    "# Type name argument could be omitted whenever the behavior is unambiguous.\n",
    "print(pa_g.number_of_edges())  # Only one edge type, the edge type argument could be omitted"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "G = dgl.heterograph({\n",
    "        ('paper', 'written-by', 'author') : data['PvsA'],\n",
    "        ('author', 'writing', 'paper') : data['PvsA'].transpose(),\n",
    "        ('paper', 'citing', 'paper') : data['PvsP'],\n",
    "        ('paper', 'cited', 'paper') : data['PvsP'].transpose(),\n",
    "        ('paper', 'is-about', 'subject') : data['PvsL'],\n",
    "        ('subject', 'has', 'paper') : data['PvsL'].transpose(),\n",
    "    })\n",
    "\n",
    "print(G)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "G.nodes['paper'].shape\n",
    "# not supported in windows\n",
    "\n",
    "# Draw the metagraph using graphviz.\n",
    "# import pygraphviz as pgv\n",
    "# def plot_graph(nxg):\n",
    "#     ag = pgv.AGraph(strict=False, directed=True)\n",
    "#     for u, v, k in nxg.edges(keys=True):\n",
    "#         ag.add_edge(u, v, label=k)\n",
    "#     ag.layout('dot')\n",
    "#     ag.draw(out_base + '/graph.png')\n",
    "# \n",
    "# plot_graph(G.metagraph)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# dia matrix: \n",
    "# 做矩阵乘积较快的方式, 适合求特征值, svd分解什么的. 但是如果矩阵的对角性不好, \n",
    "# 矩阵大小就要比其他存储方式高到不知道哪里去了. (所以random矩阵最好不要用这个方式)\n",
    "# coo/csc/csr matrix: \n",
    "# 存储方式相对的比较灵活, 也好理解. 操作重点在于读写矩阵的话用这个可能会比较好.\n",
    "# bsr matrix 分块存储\n",
    "# dok matrix: 基于键值对的方式存储的矩阵, 有点像字典的存储方式\n",
    "# lil matrix: 很有意思的增量式存储方式, 文档里面说这种方式对索引, \n",
    "# 切片操作的资瓷是非常吼的.\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# pvc shape (12499, 14) 14 conference and 12499 paper\n",
    "pvc = data['PvsC'].tocsr()\n",
    "# find all papers published in KDD, ICML, VLDB\n",
    "c_selected = [0, 11, 13]  # KDD, ICML, VLDB\n",
    "p_selected = pvc[:, c_selected].tocoo()\n",
    "# generate labels\n",
    "labels = pvc.indices # 0 KDD\n",
    "labels[labels == 11] = 1 # 1 ICML\n",
    "labels[labels == 13] = 2 # 2 VLDB\n",
    "\n",
    "labels = torch.tensor(labels).long().to(torch.device('cuda:0'))\n",
    "\n",
    "# generate train/val/test split\n",
    "pid = p_selected.row\n",
    "shuffle = np.random.permutation(pid)\n",
    "train_idx = torch.tensor(shuffle[0:800]).long().to(torch.device('cuda:0'))\n",
    "val_idx = torch.tensor(shuffle[800:900]).long().to(torch.device('cuda:0'))\n",
    "test_idx = torch.tensor(shuffle[900:]).long().to(torch.device('cuda:0'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([0, 0, 0,  ..., 7, 7, 7], device='cuda:0')\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import GCN_model as gmodel\n",
    "model = gmodel.HeteroRGCN(G, 10, 10, 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "HeteroRGCN(\n  (embed): ParameterDict(\n      (author): Parameter containing: [torch.cuda.FloatTensor of size 17431x10 (GPU 0)]\n      (paper): Parameter containing: [torch.cuda.FloatTensor of size 12499x10 (GPU 0)]\n      (subject): Parameter containing: [torch.cuda.FloatTensor of size 73x10 (GPU 0)]\n  )\n  (layer1): HeteroRGCNLayer(\n    (weight): ModuleDict(\n      (cited): Linear(in_features=10, out_features=10, bias=True)\n      (citing): Linear(in_features=10, out_features=10, bias=True)\n      (has): Linear(in_features=10, out_features=10, bias=True)\n      (is-about): Linear(in_features=10, out_features=10, bias=True)\n      (writing): Linear(in_features=10, out_features=10, bias=True)\n      (written-by): Linear(in_features=10, out_features=10, bias=True)\n    )\n  )\n  (layer2): HeteroRGCNLayer(\n    (weight): ModuleDict(\n      (cited): Linear(in_features=10, out_features=3, bias=True)\n      (citing): Linear(in_features=10, out_features=3, bias=True)\n      (has): Linear(in_features=10, out_features=3, bias=True)\n      (is-about): Linear(in_features=10, out_features=3, bias=True)\n      (writing): Linear(in_features=10, out_features=3, bias=True)\n      (written-by): Linear(in_features=10, out_features=3, bias=True)\n    )\n  )\n)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 41
    }
   ],
   "source": [
    "model.to(torch.device('cuda:0'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Loss 1.3115, Train Acc 0.1512, Val Acc 0.1500 (Best 0.1500), Test Acc 0.1382 (Best 0.1382)\n",
      "Loss 0.9693, Train Acc 0.5650, Val Acc 0.4600 (Best 0.4600), Test Acc 0.5226 (Best 0.5226)\n",
      "Loss 0.8332, Train Acc 0.5250, Val Acc 0.4300 (Best 0.4600), Test Acc 0.5075 (Best 0.5226)\n",
      "Loss 0.6297, Train Acc 0.8237, Val Acc 0.6400 (Best 0.6400), Test Acc 0.6625 (Best 0.6625)\n",
      "Loss 0.4157, Train Acc 0.9250, Val Acc 0.7400 (Best 0.7400), Test Acc 0.7194 (Best 0.7194)\n",
      "Loss 0.2462, Train Acc 0.9600, Val Acc 0.7600 (Best 0.7600), Test Acc 0.7328 (Best 0.7286)\n",
      "Loss 0.1471, Train Acc 0.9700, Val Acc 0.7500 (Best 0.7600), Test Acc 0.7395 (Best 0.7286)\n",
      "Loss 0.1027, Train Acc 0.9750, Val Acc 0.7600 (Best 0.7600), Test Acc 0.7404 (Best 0.7286)\n",
      "Loss 0.0828, Train Acc 0.9762, Val Acc 0.8000 (Best 0.8000), Test Acc 0.7462 (Best 0.7462)\n",
      "Loss 0.0724, Train Acc 0.9762, Val Acc 0.7900 (Best 0.8100), Test Acc 0.7462 (Best 0.7471)\n",
      "Loss 0.0661, Train Acc 0.9762, Val Acc 0.7800 (Best 0.8100), Test Acc 0.7462 (Best 0.7471)\n",
      "Loss 0.0606, Train Acc 0.9762, Val Acc 0.7800 (Best 0.8100), Test Acc 0.7429 (Best 0.7471)\n",
      "Loss 0.0550, Train Acc 0.9775, Val Acc 0.7800 (Best 0.8100), Test Acc 0.7437 (Best 0.7471)\n",
      "Loss 0.0492, Train Acc 0.9987, Val Acc 0.7800 (Best 0.8100), Test Acc 0.7454 (Best 0.7471)\n",
      "Loss 0.0436, Train Acc 1.0000, Val Acc 0.8100 (Best 0.8100), Test Acc 0.7513 (Best 0.7471)\n",
      "Loss 0.0384, Train Acc 1.0000, Val Acc 0.8000 (Best 0.8100), Test Acc 0.7454 (Best 0.7471)\n",
      "Loss 0.0337, Train Acc 1.0000, Val Acc 0.7900 (Best 0.8100), Test Acc 0.7462 (Best 0.7471)\n",
      "Loss 0.0298, Train Acc 1.0000, Val Acc 0.7900 (Best 0.8100), Test Acc 0.7387 (Best 0.7471)\n",
      "Loss 0.0267, Train Acc 1.0000, Val Acc 0.7800 (Best 0.8100), Test Acc 0.7353 (Best 0.7471)\n",
      "Loss 0.0241, Train Acc 1.0000, Val Acc 0.7800 (Best 0.8100), Test Acc 0.7278 (Best 0.7471)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_val_acc = 0\n",
    "best_test_acc = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    logits = model(G)\n",
    "    # The loss is computed only for labeled nodes.\n",
    "    loss = criterion(logits[train_idx], labels[train_idx])\n",
    "\n",
    "    pred = logits.argmax(1)\n",
    "    train_acc = (pred[train_idx] == labels[train_idx]).float().mean()\n",
    "    val_acc = (pred[val_idx] == labels[val_idx]).float().mean()\n",
    "    test_acc = (pred[test_idx] == labels[test_idx]).float().mean()\n",
    "\n",
    "    if best_val_acc < val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_test_acc = test_acc\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print('Loss %.4f, Train Acc %.4f, Val Acc %.4f (Best %.4f), Test Acc %.4f (Best %.4f)' % (\n",
    "            loss.item(),\n",
    "            train_acc.item(),\n",
    "            val_acc.item(),\n",
    "            best_val_acc.item(),\n",
    "            test_acc.item(),\n",
    "            best_test_acc.item(),\n",
    "        ))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "pred = logits.argmax(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([ 2.7476, -3.2470, -1.2180], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0, device='cuda:0')\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(logits[0])\n",
    "print(labels[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[('paper', 'written-by', 'author'), ('author', 'writing', 'paper'), ('paper', 'citing', 'paper'), ('paper', 'cited', 'paper'), ('paper', 'is-about', 'subject'), ('subject', 'has', 'paper')]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(G.canonical_etypes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "'Wh_is-about'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 45
    }
   ],
   "source": [
    "'Wh_%s' % 'is-about'\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0cf0fbe12a3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paper'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/dgl/view.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_n_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ntid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'h'"
     ],
     "ename": "KeyError",
     "evalue": "'h'",
     "output_type": "error"
    }
   ],
   "source": [
    "G.nodes['paper'].data['h'][0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "30003\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(sum([G.number_of_nodes(name) for name in G.ntypes]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'GCN_model' from '/tmp/pycharm_project_754/GCN_model.py'>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 55
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(gmodel)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "cpu\n",
      "cpu\n",
      "cpu\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "NormalGCN(\n  (layer1): NormalGCNLayer(\n    (weight): Linear(in_features=10, out_features=10, bias=True)\n  )\n  (layer2): NormalGCNLayer(\n    (weight): Linear(in_features=10, out_features=3, bias=True)\n  )\n)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 56
    }
   ],
   "source": [
    "G.to(torch.device('cpu'))\n",
    "nmodel = gmodel.NormalGCN(G, 10, 10, 3)\n",
    "nmodel.to(torch.device('cpu'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/data/root/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-537c880f7cd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# The loss is computed only for labeled nodes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: tensor([ 5620,    61,  8968,  6083,  5669,  5683,  2716,  1389,  7579,  6053,\n         6143,  2692,  8702,  5687,  7294, 10725,  5640,     3,  5747,  8727,\n         8461,  6044,  1546, 10743, 10241,  8737,  6821,  5688,  7311,  7232,\n         2770,  6533,    44,  1381,  1557,  2206,  6580,  8437,  5905,  5646,\n         2287,  8538,  8675,  8424,    21,  1600,  2310,  2719,  9888,  5718,\n         8969,  1555,  6003,  7594,  1297,  6171, 10703,  2298,  9876,  7580,\n        10776,  7260,  8681,  6182,  6199,  8482,    49,  7610, 10216,  7300,\n         2700,  1613,  6161,  8467,  9879,  5691,  8427,  8985,  6597,  5936,\n         1360,  9847,  7269, 10251,  1356,  6178,  6814,  2221,  5629,  2701,\n          116,  5668,  5974,  8484,  1619,  2682,  8726,  5947,  1584,  1363,\n         5625,  5980,  2731,  5679,  7297,  7567,  5910,  9850, 10720,  8963,\n         8442,  5730,  5741,  8490,  2708,  7238,  6775, 10706,  6802,  7256,\n         1596,  7595,  1639,  6767,  2699,  8942,  6544,  6785,  6527,  8672,\n         7592,  5699,  8519,  7318,  1570,  8910,  7268,  2747,  7258,  8724,\n         1310,  6574,  1334,  8665,  6026,  5700,  8446,  5693,    26,  2200,\n         5690,  6076,  5942,  1595,  8975,  7644,  1626, 10714,  8487,  8425,\n         5898,  6025,  8695,  2276,  6097,  8930,  2781,  7551,  5921,  8742,\n         2723,  2787,   107, 10730,  9828,  7299,   119,  6120,  5996,  6756,\n         2713,  6543, 10235,  7598,  7275,  6557,  1379,  6783,  1625, 10262,\n         5694,  8932,  9849,  9857,  8953,  9844,  7343,  1320,  1409,  1554,\n          108,  5731,  5714,  1635,  6751,  2294,   110,  7563,  8436,  6064,\n         6535,  8500, 10712,  2749,  8743, 10713,  5964,    38,  8474,  7214,\n         9892,  6124,  8922,  6522, 10747,  8498,  1652,  2230,  1327,  7272,\n         8979,  1605,  5997,  8491,  6562,  5716,  6005, 10800,  5916,  8902,\n         6588,  6590,  6539,  5653,  8458,  6566,  8689,  8931,  9817,  1325,\n         1383,  1335,  7271,  7604,  6796,  7265,  8749,  6565,  6774,   109,\n         6050,  6062,  2724, 10741,  6112,  1298,  7230,  5914,  6004,  7600,\n         2248,  6051,  1549,  9843, 12420,  2773,  8911,  2241,  2702,  1564,\n         7561,  6541,  8951,  6800,  2312,  6832,  8721,   120,  9830,  7608,\n         1636,  6134,  5675,  5638,    91,  6096,  9821,  7552,  1370,  5720,\n         6117,  6776,  8431,  2215,  6135,  6130,  8692,  2754, 10782, 10226,\n         6798,  2233,  9864,  7273,  8978, 10774,  6139,  1598,  2722,  2301,\n         1651,  8693,  5998,  1580,  6039,  6569,  2234,  2219,  6115,  5647,\n         7616,  5933,  1556,  2259,  1594,  8943,  5737,    18,  8729,  9831,\n         6000,  8929,  7213,    89,  7339,  6766,  5911,  5654,  8701,  5736,\n         6017, 10269,  1347,  2205,  8959,  5680,  8706,  6755, 10749,  6606,\n         1387,   111,  1558,  1408,    40,  6156,    16,  5672,  5979,  6059,\n         2678,  8486,  1643,  2256,  6172,  1585,  6833,  6157,    33,  7270,\n         8708,  6830,  2759,  7309,  2235,  2767, 10218,  2306,  2216,  7606,\n         1405,  5729,  2214,  1397,    53,  7246,    13,  7313,  6032,  7534,\n         2776,  8426,  6136,  9871,  9884,     0,  6109,  7587,  8987,  1322,\n        10769,  7303,  1620,  6092, 10732,  6204,  8535,     5,    78,  7323,\n         8454,  1366,  5711,  2260,  6748,   113,  8733,  6558,  5978,  2254,\n         1606,  6007,  5924,  9829,  9882,  1403,  6128, 10721,  9901,  9866,\n         1304,  1399,  1354,  2684,  7229,  2240,  1313,  6185,  7325,  1572,\n         7247,  2224,  6205,  6596,  7638, 10238,   102,  7307,  5906,  5943,\n        10239,  9877,  7609,  8481,  5619,    62,  8898,  5670,  5928,  9825,\n         7590,    94,    48,  8703,   121,  1404,  6771,  6058,  8495,  2313,\n         7582,  6012,  6788,   122,  6524,  1610, 10793,  7593,  7573,  2761,\n         7219,  5895, 10740,  8673,    60,  2686,  6747, 10754,  7285,  6810,\n         8905,  8900,  5971,  2792, 10708,  2182, 10253,  6521,  6187,  5719,\n         6164,    39,  5631,  7599,  1413,  8469,  1301,  8452,  6122,  6754,\n         6780,  7344,  5935,  9891,  6786,  6063,  8914,  5624,    11, 10763,\n           90,  1617,  5610, 10728, 10271,  6758,  1621,  6127,  1372,  2227,\n        10750,  7577,  5671, 10792,  7288,  5704,  2291,  8974,  2693,  2748,\n         2788,  8741,  9854,  6108,  7529,  2225,  8980,  7283,  1369,    72,\n         5627, 10232, 10775,  8682,  8894,  6074,  5615,  6816,  2231,  6202,\n        10765,     8,  8674,  6132,  7576,  8448,  5722,  6123,  8433,    63,\n         6552,  8704, 10724,  9862,  8542,  1373,    42,  9852,  8504,  7578,\n         8545,  6073,  2237, 10762,  6616,  6099,  6820,  7570,  6177,  8477,\n         6555, 10233,  2290,  5641,  5706,  6180,  5715,  5963,  6792,  8940,\n         2696,  6054,  7612,  8732,  2288,  8899,  7643,  8676,  2183,  5643,\n        10736,  5907,  8518,  6787,  6065,   115,  8536,  8440,  1567,  2283,\n         5648,  8909,    41,  5673,  6809,  1311,  8973,  6793,  8982,  1392,\n         9899,  5635,  7620,    19,  7621,  9874,    56,  6002,  5681,  5926,\n         6757,  8468,  1394,  2740,    58,  5712,  5639,     2, 10760,  6035,\n         1586,  7221,  9835, 10727,  8945,  2258,  7264,  6208, 10711,   118,\n         1648,  6813,  7210,  5697,  6559,  7535,  8944,  6608,  8506,  1587,\n        10780,  2286,  5685,  5991,  7586,  1655,  6181,  8520,  7639,  8515,\n         8543,  2271,  6188,  5611,  8476,  8460,    98,  6010,  6189,  5976,\n         6768,  7225,  7585,  6100,  6056,  1593,  8457,  6811,  8916,  6166,\n         9860,  1378,  7626,  1559,  7237,    50,  5707,  7541,  6168,  8447,\n         7236,  8497, 10719, 10731,  6022,  6838,  2274,  6069,    45,  2212,\n         8432,  1589, 10746, 10273,    97,  6006,  7633,  8494,  8723, 10225,\n         7631, 10244,  8892,  6829,  7314,  8962,  5692,  2305,  8462,  5645,\n        10220,    47,  6031,  8740,  7296,    59, 10267,  9855,  1351,  6140,\n         5934,  5637,  6021,  8530,  8917,  6525,  8926,  6753,  6170,  2266,\n         5999,  1560,  2786,  2779, 10784,  8453,  6118,  6043,  2738,  7539,\n         5989,  6781, 10227,  9859,  5686,  5616,  5904,  7642,    36,  7259,\n         1638, 10783,  1545,  6831,  2295, 10801,  9846,  2697,  8540,  8748])"
     ],
     "ename": "KeyError",
     "evalue": "tensor([ 5620,    61,  8968,  6083,  5669,  5683,  2716,  1389,  7579,  6053,\n         6143,  2692,  8702,  5687,  7294, 10725,  5640,     3,  5747,  8727,\n         8461,  6044,  1546, 10743, 10241,  8737,  6821,  5688,  7311,  7232,\n         2770,  6533,    44,  1381,  1557,  2206,  6580,  8437,  5905,  5646,\n         2287,  8538,  8675,  8424,    21,  1600,  2310,  2719,  9888,  5718,\n         8969,  1555,  6003,  7594,  1297,  6171, 10703,  2298,  9876,  7580,\n        10776,  7260,  8681,  6182,  6199,  8482,    49,  7610, 10216,  7300,\n         2700,  1613,  6161,  8467,  9879,  5691,  8427,  8985,  6597,  5936,\n         1360,  9847,  7269, 10251,  1356,  6178,  6814,  2221,  5629,  2701,\n          116,  5668,  5974,  8484,  1619,  2682,  8726,  5947,  1584,  1363,\n         5625,  5980,  2731,  5679,  7297,  7567,  5910,  9850, 10720,  8963,\n         8442,  5730,  5741,  8490,  2708,  7238,  6775, 10706,  6802,  7256,\n         1596,  7595,  1639,  6767,  2699,  8942,  6544,  6785,  6527,  8672,\n         7592,  5699,  8519,  7318,  1570,  8910,  7268,  2747,  7258,  8724,\n         1310,  6574,  1334,  8665,  6026,  5700,  8446,  5693,    26,  2200,\n         5690,  6076,  5942,  1595,  8975,  7644,  1626, 10714,  8487,  8425,\n         5898,  6025,  8695,  2276,  6097,  8930,  2781,  7551,  5921,  8742,\n         2723,  2787,   107, 10730,  9828,  7299,   119,  6120,  5996,  6756,\n         2713,  6543, 10235,  7598,  7275,  6557,  1379,  6783,  1625, 10262,\n         5694,  8932,  9849,  9857,  8953,  9844,  7343,  1320,  1409,  1554,\n          108,  5731,  5714,  1635,  6751,  2294,   110,  7563,  8436,  6064,\n         6535,  8500, 10712,  2749,  8743, 10713,  5964,    38,  8474,  7214,\n         9892,  6124,  8922,  6522, 10747,  8498,  1652,  2230,  1327,  7272,\n         8979,  1605,  5997,  8491,  6562,  5716,  6005, 10800,  5916,  8902,\n         6588,  6590,  6539,  5653,  8458,  6566,  8689,  8931,  9817,  1325,\n         1383,  1335,  7271,  7604,  6796,  7265,  8749,  6565,  6774,   109,\n         6050,  6062,  2724, 10741,  6112,  1298,  7230,  5914,  6004,  7600,\n         2248,  6051,  1549,  9843, 12420,  2773,  8911,  2241,  2702,  1564,\n         7561,  6541,  8951,  6800,  2312,  6832,  8721,   120,  9830,  7608,\n         1636,  6134,  5675,  5638,    91,  6096,  9821,  7552,  1370,  5720,\n         6117,  6776,  8431,  2215,  6135,  6130,  8692,  2754, 10782, 10226,\n         6798,  2233,  9864,  7273,  8978, 10774,  6139,  1598,  2722,  2301,\n         1651,  8693,  5998,  1580,  6039,  6569,  2234,  2219,  6115,  5647,\n         7616,  5933,  1556,  2259,  1594,  8943,  5737,    18,  8729,  9831,\n         6000,  8929,  7213,    89,  7339,  6766,  5911,  5654,  8701,  5736,\n         6017, 10269,  1347,  2205,  8959,  5680,  8706,  6755, 10749,  6606,\n         1387,   111,  1558,  1408,    40,  6156,    16,  5672,  5979,  6059,\n         2678,  8486,  1643,  2256,  6172,  1585,  6833,  6157,    33,  7270,\n         8708,  6830,  2759,  7309,  2235,  2767, 10218,  2306,  2216,  7606,\n         1405,  5729,  2214,  1397,    53,  7246,    13,  7313,  6032,  7534,\n         2776,  8426,  6136,  9871,  9884,     0,  6109,  7587,  8987,  1322,\n        10769,  7303,  1620,  6092, 10732,  6204,  8535,     5,    78,  7323,\n         8454,  1366,  5711,  2260,  6748,   113,  8733,  6558,  5978,  2254,\n         1606,  6007,  5924,  9829,  9882,  1403,  6128, 10721,  9901,  9866,\n         1304,  1399,  1354,  2684,  7229,  2240,  1313,  6185,  7325,  1572,\n         7247,  2224,  6205,  6596,  7638, 10238,   102,  7307,  5906,  5943,\n        10239,  9877,  7609,  8481,  5619,    62,  8898,  5670,  5928,  9825,\n         7590,    94,    48,  8703,   121,  1404,  6771,  6058,  8495,  2313,\n         7582,  6012,  6788,   122,  6524,  1610, 10793,  7593,  7573,  2761,\n         7219,  5895, 10740,  8673,    60,  2686,  6747, 10754,  7285,  6810,\n         8905,  8900,  5971,  2792, 10708,  2182, 10253,  6521,  6187,  5719,\n         6164,    39,  5631,  7599,  1413,  8469,  1301,  8452,  6122,  6754,\n         6780,  7344,  5935,  9891,  6786,  6063,  8914,  5624,    11, 10763,\n           90,  1617,  5610, 10728, 10271,  6758,  1621,  6127,  1372,  2227,\n        10750,  7577,  5671, 10792,  7288,  5704,  2291,  8974,  2693,  2748,\n         2788,  8741,  9854,  6108,  7529,  2225,  8980,  7283,  1369,    72,\n         5627, 10232, 10775,  8682,  8894,  6074,  5615,  6816,  2231,  6202,\n        10765,     8,  8674,  6132,  7576,  8448,  5722,  6123,  8433,    63,\n         6552,  8704, 10724,  9862,  8542,  1373,    42,  9852,  8504,  7578,\n         8545,  6073,  2237, 10762,  6616,  6099,  6820,  7570,  6177,  8477,\n         6555, 10233,  2290,  5641,  5706,  6180,  5715,  5963,  6792,  8940,\n         2696,  6054,  7612,  8732,  2288,  8899,  7643,  8676,  2183,  5643,\n        10736,  5907,  8518,  6787,  6065,   115,  8536,  8440,  1567,  2283,\n         5648,  8909,    41,  5673,  6809,  1311,  8973,  6793,  8982,  1392,\n         9899,  5635,  7620,    19,  7621,  9874,    56,  6002,  5681,  5926,\n         6757,  8468,  1394,  2740,    58,  5712,  5639,     2, 10760,  6035,\n         1586,  7221,  9835, 10727,  8945,  2258,  7264,  6208, 10711,   118,\n         1648,  6813,  7210,  5697,  6559,  7535,  8944,  6608,  8506,  1587,\n        10780,  2286,  5685,  5991,  7586,  1655,  6181,  8520,  7639,  8515,\n         8543,  2271,  6188,  5611,  8476,  8460,    98,  6010,  6189,  5976,\n         6768,  7225,  7585,  6100,  6056,  1593,  8457,  6811,  8916,  6166,\n         9860,  1378,  7626,  1559,  7237,    50,  5707,  7541,  6168,  8447,\n         7236,  8497, 10719, 10731,  6022,  6838,  2274,  6069,    45,  2212,\n         8432,  1589, 10746, 10273,    97,  6006,  7633,  8494,  8723, 10225,\n         7631, 10244,  8892,  6829,  7314,  8962,  5692,  2305,  8462,  5645,\n        10220,    47,  6031,  8740,  7296,    59, 10267,  9855,  1351,  6140,\n         5934,  5637,  6021,  8530,  8917,  6525,  8926,  6753,  6170,  2266,\n         5999,  1560,  2786,  2779, 10784,  8453,  6118,  6043,  2738,  7539,\n         5989,  6781, 10227,  9859,  5686,  5616,  5904,  7642,    36,  7259,\n         1638, 10783,  1545,  6831,  2295, 10801,  9846,  2697,  8540,  8748])",
     "output_type": "error"
    }
   ],
   "source": [
    "labels = torch.tensor(labels).long()\n",
    "\n",
    "# generate train/val/test split\n",
    "pid = p_selected.row\n",
    "shuffle = np.random.permutation(pid)\n",
    "train_idx = torch.tensor(shuffle[0:800]).long()\n",
    "val_idx = torch.tensor(shuffle[800:900]).long()\n",
    "test_idx = torch.tensor(shuffle[900:]).long()\n",
    "opt = torch.optim.Adam(nmodel.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_val_acc = 0\n",
    "best_test_acc = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    logits = nmodel(G)\n",
    "    # The loss is computed only for labeled nodes.\n",
    "    loss = criterion(logits[train_idx], labels[train_idx])\n",
    "\n",
    "    pred = logits.argmax(1)\n",
    "    train_acc = (pred[train_idx] == labels[train_idx]).float().mean()\n",
    "    val_acc = (pred[val_idx] == labels[val_idx]).float().mean()\n",
    "    test_acc = (pred[test_idx] == labels[test_idx]).float().mean()\n",
    "\n",
    "    if best_val_acc < val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_test_acc = test_acc\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print('Loss %.4f, Train Acc %.4f, Val Acc %.4f (Best %.4f), Test Acc %.4f (Best %.4f)' % (\n",
    "            loss.item(),\n",
    "            train_acc.item(),\n",
    "            val_acc.item(),\n",
    "            best_val_acc.item(),\n",
    "            test_acc.item(),\n",
    "            best_test_acc.item(),\n",
    "        ))\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}